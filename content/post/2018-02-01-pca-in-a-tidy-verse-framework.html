---
title: PCA in a tidy(verse) framework
author: Tyler Bradley
date: '2018-02-01'
slug: pca-in-a-tidy-verse-framework
categories:
  - R
tags:
  - tidyverse
  - PCA
  - dplyr
  - rstats
  - purrr
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The other day, a <a href="https://community.rstudio.com/t/tidyverse-solutions-for-factor-analysis-principal-component-analysis/4504">question</a> was posted on <a href="https://community.rstudio.com/">RStudio Community</a> about performing Principle Component Analysis (PCA) in a <a href="https://www.tidyverse.org/">tidyverse</a> workflow. Conveniently, I had literally just worked through this process the day before and was able to post an <a href="https://community.rstudio.com/t/tidyverse-solutions-for-factor-analysis-principal-component-analysis/4504/2">answer</a>. While most questions and answers are good as they are on forum sites, I thought this one might be worth exploring a little more since using the tidyverse framework makes PCA much easier, in my opinion.</p>
<p>PCA is a multi-variate statistical technique for dimension reduction. Essentially, it allows you to take a data set that has n continuous variables and relate them through n orthogonal dimensions. This is a method of unsupervised learning that allows you to better understand the variability in the data set and how different variables are related. While there are the same number of principle components created as there are variables (assuming you have more observations than variables-but that is another issue), each principle component explains the maximum possible variation in the data conditional on it being orthogonal, or perpendicular, to the previous principle components.</p>
<p>In my answer, I used the <code>iris</code> data set to demonstrate how PCA can be done in the tidyverse workflow. For this post, I will be using the <code>USArrests</code> data set that was used in <a href="http://www-bcf.usc.edu/~gareth/ISL/index.html">An Introduction to Statistical Thinking</a> by Gareth James et. al. In this book, they work through a PCA and focus on the statistics and explanations behind PCA. This is how I learned how to do PCA and would highly recommend it if you are unfamiliar with the topic. In this blog post, my focus will be more on implementing the PCA in the tidyverse framework.</p>
</div>
<div id="exploring-the-data" class="section level1">
<h1>Exploring the data</h1>
<p>Before we dive in to the analysis, we want to explore our data set and become familiar with it.</p>
<pre class="r"><code>library(tidyverse)
library(broom)
library(knitr)
library(ggfortify)</code></pre>
<pre class="r"><code>USArrests %&gt;% head() %&gt;% knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Murder</th>
<th align="right">Assault</th>
<th align="right">UrbanPop</th>
<th align="right">Rape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td align="right">13.2</td>
<td align="right">236</td>
<td align="right">58</td>
<td align="right">21.2</td>
</tr>
<tr class="even">
<td>Alaska</td>
<td align="right">10.0</td>
<td align="right">263</td>
<td align="right">48</td>
<td align="right">44.5</td>
</tr>
<tr class="odd">
<td>Arizona</td>
<td align="right">8.1</td>
<td align="right">294</td>
<td align="right">80</td>
<td align="right">31.0</td>
</tr>
<tr class="even">
<td>Arkansas</td>
<td align="right">8.8</td>
<td align="right">190</td>
<td align="right">50</td>
<td align="right">19.5</td>
</tr>
<tr class="odd">
<td>California</td>
<td align="right">9.0</td>
<td align="right">276</td>
<td align="right">91</td>
<td align="right">40.6</td>
</tr>
<tr class="even">
<td>Colorado</td>
<td align="right">7.9</td>
<td align="right">204</td>
<td align="right">78</td>
<td align="right">38.7</td>
</tr>
</tbody>
</table>
<p>Looking at the first 6 rows using the <code>head()</code> function, we can see that each row is a state and and each column is a variable. Looking at <code>?USArrests</code>, we can see that the column descriptions are as follows:</p>
<ol style="list-style-type: decimal">
<li>Murder - the number of murder arrests per 100,000 people in a given state</li>
<li>Assault - the number of assault arrests per 100,000 people in a given state</li>
<li>UrbanPop - a numeric percentage of the urban population per state (i.e. the percentage of the state’s population that lives in cities)</li>
<li>Rape - the number of rape arrests per 100,000 people in a given state</li>
</ol>
<p>Now, that we see how the data set is set up, let’s try to visualize the data as it is. Before we do that, let’s convert the data set to a <code>tibble</code>. Since tibbles do not support rownames, we will have to convert them to their own column with <code>rownames_to_column</code>.</p>
<pre class="r"><code>us_arrests &lt;- USArrests %&gt;% 
  rownames_to_column(var = &quot;state&quot;) %&gt;% 
  # I prefer column names to be all lowercase so I am going to change them here
  rename_all(tolower) %&gt;% 
  as_tibble()

us_arrests</code></pre>
<pre><code>## # A tibble: 50 x 5
##    state       murder assault urbanpop  rape
##    &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;
##  1 Alabama      13.2      236       58  21.2
##  2 Alaska       10.0      263       48  44.5
##  3 Arizona       8.10     294       80  31.0
##  4 Arkansas      8.80     190       50  19.5
##  5 California    9.00     276       91  40.6
##  6 Colorado      7.90     204       78  38.7
##  7 Connecticut   3.30     110       77  11.1
##  8 Delaware      5.90     238       72  15.8
##  9 Florida      15.4      335       80  31.9
## 10 Georgia      17.4      211       60  25.8
## # ... with 40 more rows</code></pre>
<p>Let’s take a look at murder arrest rates in each of the stats.</p>
<pre class="r"><code>us_arrests %&gt;% 
  ggplot(aes(state, murder)) + 
  geom_bar(stat = &quot;identity&quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.4)) +
  labs(y = &quot;Murder Arrest Rate per 100,000 people&quot;,
       x = &quot;State&quot;,
       title = &quot;Murder rate in each state of the USA&quot;)</code></pre>
<p><img src="/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/murder-bar-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>You can see that Georgia has the highest murder rate, followed by Mississippi, Florida, and Louisiana. Let’s try one more.</p>
<pre class="r"><code>us_arrests %&gt;% 
  gather(key = crime, value = rate, c(murder, assault, rape)) %&gt;% 
  ggplot(aes(urbanpop, rate, color = crime)) + 
  facet_wrap(~crime, scales = &quot;free_y&quot;, ncol = 1) +
  geom_point() + 
  geom_smooth(se = FALSE, method = &quot;lm&quot;) +
  theme_bw() + 
  labs(x = &quot;Percentage Urban Population&quot;,
       y = &quot;Arrest Rate per 100,000 people&quot;,
       title = &quot;Arrest rate vs percentage urban population&quot;) +
  theme(legend.title = element_blank(),
        legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/tidy-us-arrests-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>This figure is a slightly more informative than the last one. In this figure, each crime rate is plotted against percentage urban population. Simple linear models are fit for each of the different crime types to see if any pattern can be seen in the data. There appears to be an increase in assault arrests as urban population grows, however, there is <em>a lot</em> of variability around the line of best fit. Rape arrest rates follow the linear model much closer than the others but there is still a lot of variability. On the other hand, murder arrest rates seem to be unchanged based on urban population.</p>
<p>Now we could certainly do correlations, multiple linear regressions, or fit other types of models and would likely gain some useful insights, but instead let’s focus on the PCA.</p>
</div>
<div id="pca-in-the-tidyverse-framework" class="section level1">
<h1>PCA in the tidyverse framework</h1>
<p>Now, when I first fit PCA models in R, I found myself with an unmanageable number of variables and to track and maintain. This can make the process overwhelming and can make you lose track of information. Luckily, using the <code>tidyverse</code> and the <code>broom</code> package, we can solve these issues much more easily. In order to run the model in the tidyverse framework, we will use the <code>nest()</code> function along with the <code>mutate</code> and <code>map</code> combo to operate on the nested columns.</p>
<pre class="r"><code>us_arrests_pca &lt;- us_arrests %&gt;% 
  nest() %&gt;% 
  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-state), 
                                  center = TRUE, scale = TRUE)),
         pca_aug = map2(pca, data, ~augment(.x, data = .y)))

us_arrests_pca</code></pre>
<pre><code>## # A tibble: 1 x 3
##   data              pca          pca_aug               
##   &lt;list&gt;            &lt;list&gt;       &lt;list&gt;                
## 1 &lt;tibble [50 x 5]&gt; &lt;S3: prcomp&gt; &lt;data.frame [50 x 10]&gt;</code></pre>
<p>As you can see, we now have a tibble with one row and three columns. The first is our original data set, the second is the <code>prcomp</code> object and the third is a data frame containing the principle component values for each observation. Now we have everything we need to evaluate the results of the model. First, it is important to look at how much variance is explained by each principle component. This will tell you how many of the components you need to look at when analyzing the results. To do this, you can use the data in the <code>pca_aug</code> column of our <code>us_arrests_pca</code> tibble along with some <code>dplyr</code> functions.</p>
<pre class="r"><code>var_exp &lt;- us_arrests_pca %&gt;% 
  unnest(pca_aug) %&gt;% 
  summarize_at(.vars = vars(contains(&quot;PC&quot;)), .funs = funs(var)) %&gt;% 
  gather(key = pc, value = variance) %&gt;% 
  mutate(var_exp = variance/sum(variance),
         cum_var_exp = cumsum(var_exp),
         pc = str_replace(pc, &quot;.fitted&quot;, &quot;&quot;))

var_exp</code></pre>
<pre><code>## # A tibble: 4 x 4
##   pc    variance var_exp cum_var_exp
##   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;
## 1 PC1      2.48   0.620        0.620
## 2 PC2      0.990  0.247        0.868
## 3 PC3      0.357  0.0891       0.957
## 4 PC4      0.173  0.0434       1.00</code></pre>
<p>We can see that the first principle component explains 62% of the variation and that the second principle component explains 25% of the remaining variation. Together, we can see that they explain 87% of the variance in the data set. As a general rule of thumb, you want to look at enough principle components to explain ~90% of your data’s variability.</p>
<p>Another way of assessing this is to plot the variance explained and the cumulative variance explained and look for the “elbow” in the graph. You can do that like this:</p>
<pre class="r"><code>var_exp %&gt;% 
  rename(
    `Variance Explained` = var_exp,
    `Cumulative Variance Explained` = cum_var_exp
  ) %&gt;% 
  gather(key = key, value = value, `Variance Explained`:`Cumulative Variance Explained`) %&gt;% 
  ggplot(aes(pc, value, group = key)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~key, scales = &quot;free_y&quot;) +
  theme_bw() +
  lims(y = c(0, 1)) +
  labs(y = &quot;Variance&quot;,
       title = &quot;Variance explained by each principle component&quot;)</code></pre>
<p><img src="/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/var-exp-graph-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Since the majority of the variance is explained by the first two principle components, let’s plot them against each other. Luckily, this is made easy by the <code>ggplot2</code> and <code>ggfortify</code> packages which gives an <code>autoplot</code> method for prcomp objects. Conveniently, we still have our <code>prcomp</code> object stored in the our <code>us_arrests_pca</code> tibble along with our original data. This is important as the original data is needed to add labels and/or colors to your ggplot based on discrete variables not included in the PCA. Using the same <code>mutate</code> and <code>map</code> combo as before, along with the handy <code>dplyr::pull</code> function, we can plot this graph. <code>autoplot.prcomp()</code> can take any arguments that can be passed to <code>ggbiplot()</code>, so to see all of your options use <code>?ggbiplot</code>.</p>
<pre class="r"><code>us_arrests_pca %&gt;%
  mutate(
    pca_graph = map2(
      .x = pca,
      .y = data,
      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,
                 laodings.label.repel = TRUE,
                 data = .y, label = TRUE,
                 label.label = &quot;state&quot;,
                 label.repel = TRUE) +
        theme_bw() +
        labs(x = &quot;Principle Component 1&quot;,
             y = &quot;Principle Component 2&quot;,
             title = &quot;First two principle components of PCA on USArrests dataset&quot;)
    )
  ) %&gt;%
  pull(pca_graph)</code></pre>
<pre><code>## [[1]]</code></pre>
<p><img src="/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/pca-plot-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>If you notice the <code>[[1]]</code> above the graph, this method would automatically print each graph in the <code>pca_graph</code> column. This means that if you fit multiple pca graphs onto a grouped data set then this would automatically print all of the figures for you in one command. Another important thing to notice is the the <code>autoplot()</code> function simply returns a <code>gg</code> object and that can be extended just like any other <code>ggplot</code>.</p>
<p>Looking at this figure, we are getting a lot more information about how these variables are related. For example, we can see that murder, rape, and assault arrest rates all have similar values alone the first principle component, which indicates that they are correlated with one another. We may have been able to figure that out using more basic methods, but this method allows us to see it in one figure. In addition, we are able to see how each of the observations in the data (i.e. states) relates to the variables. For example, California is characterized by high arrest rates (1st principle component) and high urban population (2nd principle component).</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>PCA can provide an analysis with a lot of information about their data set and can give valuable insights into potentially unseen relationships between the observations and the variables. By performing this analysis in the tidyverse framework, we can easily extend this model by using the modelling capabilities of the tidyverse. You can see some of these <a href="http://r4ds.had.co.nz/many-models.html">here</a> and <a href="http://www.business-science.io/code-tools/2017/10/24/demo_week_timetk.html">here</a>. I believe that this framework allows for much more flexibility in your analysis and how you use the data from the model, which will enable you to gather more from your data in a faster and convenient way.</p>
<p>Please let me know what you think of performing PCA in the tidyverse framework!</p>
</div>
